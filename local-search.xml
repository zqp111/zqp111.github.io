<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>VAE</title>
    <link href="/2022/03/31/VAE/"/>
    <url>/2022/03/31/VAE/</url>
    
    <content type="html"><![CDATA[<h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><p>本篇从VAE开始讲起，主要来分析VAE的作用，限制，以及后来的一些改进模型。主要参考 <a href="https://kexue.fm/">科学空间</a>、<a href="https://panxiaoxie.cn/">panxiaoxie</a>.</p><h3 id="AE"><a href="#AE" class="headerlink" title="AE"></a>AE</h3><p>Auto-Encoder 模型是一种非常常见的学习高维数据的低维表示的方式，我们通常训练一个Auto-Encoder模型来让其自动的学习图像的低维表示。</p><p>但是其只是将一幅图像映射到了隐空间中的一个向量上（一个点），不同的图像映射到不同的隐空间中的点。（这个与VQ-VAE还是不一样，虽然大家都是离散的向量）。其并不具备生成没见过的图像的性质，即如果在隐空间中随便找一个vector，然后使用decoder去解码，大概率是得不到想要的图像的。</p><p>这里<strong>不能进行生成的一个解释</strong>是：</p><p>对于AE模型，由于其encoder和decoder都是神经网络，使用了非线性变化的过程，其隐空间变量和原始空间之间可能并不能找到一个合理的关系，导致随机采样得到的解码结果往往是乱码或十分模糊。</p><h3 id="VAE-1"><a href="#VAE-1" class="headerlink" title="VAE"></a>VAE</h3><p>上面说到，AE模型中的每个样本，都会映射到隐空间的一个点上，这导致其在隐空间中随机采样一个点并不能得到好的生成效果。VAE的想法与此不同，其不再将每个样本映射到隐空间的一个点上，而是将每一个样本都映射到一个分布上去，然后再从这个分布上采样一个点，去重建原本的样本，这样就做到了可以在隐空间中采样去生成图像。</p><p><img src="image/2584918486.png" alt="事实上，vae是为每个样本构造专属的正态分布，然后采样来重构"></p><p>图像来源<a href="https://kexue.fm/archives/5253">科学空间</a></p><p>生成模型的目标在于找到数据样本的分布，即我们给定一个数据集$X = {X_1, X_2, … X_n}$, 如果我们能找到一个分布$p(X)$, 其表示了数据集$X$的分布，那么我们就可以直接在$p(X)$中采样，得到所有的数据，包括数据集$X$中没有出现过的数据。然而这个目标是无法实现的，我们可以通过另一种方式来实现：</p><p>$$p(X) = \sum_m p(m)p(x|m) = \int_zp(z)p(x|z)dz$$</p><p>上面，$m \sim p(m), x|m \sim N(\mu^m, \delta^m)$。经过这样的变化，我们将存在大量失真区域的隐空间，转变为连续的隐空间。这里，VAE的想法就非常自然了，既然AE模型得到的只是隐空间中的一个点，那么VAE直接将其映射到一个正态分布，使得其能够包含整个隐空间。</p><p>我们可以看出来，整个模型中包含：</p><p>$$p(z) \sim N(0, 1), $$先验概率，即假设噪声z的分布服从标准正态分布。</p><p>$$p(x|z)$$， 似然概率，其对应于VAE中的decoder模型</p><p>$$q(z|x)$$，后验分布，对应于VAE中的encoder模型，假设也为正态分布。</p><p>我们从上可以看出，因为我们已经假设p(z)，那么我们只要最大似然就可以很容易的优化$$p(X)$$，然而由于我们不可能去采样所有的$p(z)$，因此这种方法不可行。</p><p>那么我们使用$q(z|x)$来辅助求解$p(x|z)$。我们的目标在于最大化$log ,p(X)$</p><p>我们可以推导：</p><p><img src="image/VAE.png" alt="[公式]"></p><p>右边一项由于KL函数的性质，恒大于0，因此我们找到了$log , P(x)$的一个下界：</p><p><img src="image/ELBO.png" alt="[公式]"></p><p>我们将其记做$L_b$，那么原式可以写作：</p><p>$$logP(x) = L_b+KL(q(z|x)||P(z|x))$$</p><p>根据公式，$P(x) = \int_zP(z)P(x|z)dz$，当我们固定了$P(x|z)$，那么$P(x)$就是固定的，而此时我们可以通过调整$q(z|x)$即编码器，来使得KL散度项趋近于零。也就是说，L_b项可以代表我们想要求的$logP$。</p><p>那么我们的优化目标就可以变为优化$L_b$。</p><p><img src="image/ELBO2.png" alt="[公式]"></p><p>对于第一项，由于我们有假设$q \sim N(\mu, \delta^2), P(z) \sim N(0, 1)$，可以根据KL散度公式进行展开，推导过程借用：</p><p><img src="image/KL.png" alt="img"></p><p>对于第二项，我们可以将其表示为</p><p>$$\int_zq(z|x)logP(x|z)dz = E_{q(z|x)}[logP(x|z)]$$</p><p>可以将其理解为AE的损失函数。</p><p>实际上我们从另一个角度理解这个损失函数，可以更具体一些。即我们一个朴素的思想还是，使得我们的重构误差最小，即上式中的第二项。但是VAE为什么还要有其他的loss呢？直观的，我们如果只包含一个重构loss，那么由于encoder得出的是一个均值和方差，为了让loss更小，模型学习的方法肯定会让方差等于0，也就是失去了随机噪声，VAE就会退化为AE模型。那么我们另一个直观的想法，既然后验分布需要一定的噪声，那么我们就强迫其向标准正态靠近，因此可以加一个loss来衡量它和标准正态的距离，这个衡量函数就是KL散度。</p><p>代码实现：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VariationalEncoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims</span>):</span>        <span class="hljs-built_in">super</span>(VariationalEncoder, self).__init__()        self.linear1 = nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">512</span>)        self.linear2 = nn.Linear(<span class="hljs-number">512</span>, latent_dims)        self.linear3 = nn.Linear(<span class="hljs-number">512</span>, latent_dims)                self.kl = <span class="hljs-number">0</span>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        x = torch.flatten(x, start_dim=<span class="hljs-number">1</span>)        x = F.relu(self.linear1(x))        mu =  self.linear2(x)        sigma = torch.exp(self.linear3(x))                z = mu + sigma*torch.randn_like(sigma)        self.kl = <span class="hljs-number">0.5</span>*(sigma**<span class="hljs-number">2</span> + mu**<span class="hljs-number">2</span> - torch.log(sigma) - <span class="hljs-number">1</span>).<span class="hljs-built_in">sum</span>()        <span class="hljs-keyword">return</span> z<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims</span>):</span>        <span class="hljs-built_in">super</span>(Decoder, self).__init__()        self.linear1 = nn.Linear(latent_dims, <span class="hljs-number">512</span>)        self.linear2 = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">784</span>)            <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, z</span>):</span>        x_hat = F.relu(self.linear1(z))        x_hat = torch.sigmoid(self.linear2(x_hat))        <span class="hljs-keyword">return</span> x_hat.reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VariationalAutoencoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims</span>):</span>        <span class="hljs-built_in">super</span>(VariationalAutoencoder, self).__init__()        self.encoder = VariationalEncoder(latent_dims)        self.decoder = Decoder(latent_dims)        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        z = self.encoder(x)        <span class="hljs-keyword">return</span> self.decoder(z)</code></pre><h3 id="VQVAE"><a href="#VQVAE" class="headerlink" title="VQVAE"></a>VQVAE</h3><p>不同于VAE，其不再将图片编码到一个连续的空间，而是像AE一样的离散空间。其过程为</p><p>codebook C: ${ (k, e(k)}_{k \in [K]}$, K表示codebook的大小, $e(k) \in R^{n_z}$</p><p>输入图片：X, encoder：E, decoder: G, </p><p>给定一个vector $z \in R^{n_z}$, 使用$Q(z; C)$来表示$z$的$VQ$，其计算也十分简单，为：</p><p>$$ Q(z; C) = argmin_{k \in [K]} ||z-e(k)||_2^2$$</p><p>那么对于整张图片$X \in R^{H_o \times W_o \times 3}$，其在输入编码器后得到</p><p>$$Z = E(X) \in R^{H \times W \times n_z}$$</p><p>这里我们使用VQ对Z进行编码，得到</p><p>$$M_{hw} = Q(Z_{HW}; C), M_{hw} \in [K]^{hw}$$</p><p>即使用codebook的index来表示图片。之后就可以使用 quantized 特征$\hat{Z}$来代替原始的feature map Z:</p><p>$$\hat{Z} = e(M_{hw}) \in R^{H \times W \times n_z}$$</p><p>之后即可使用$\hat{z}$ 送入G中对X进行重建</p><p>$$\hat{X} = G(\hat{Z}) \in R^{H_o \times W_o \times 3}$$</p><p><strong>梯度反传</strong></p><p>我们可以看到，在前向求$\hat{Z}$时，用到了$arg min$操作，这个操作本身是没有梯度的，如果我们在优化时，使用着一个loss：</p><p>$$||x-decoder(\hat{z})||^2_2$$</p><p>其梯度不会更新encoder，梯度在$argmin$那里就停止了。这里VQVAE使用了Straight-Through Estimator方法，其思想十分简单，就是在前向传播时使用自己想要得变量进行计算，而在反向chanson时，使用自己为其设计的梯度。在这里，VQVAE就是用了$z$的梯度来代替$\hat{z}$：</p><p>$$||x-decoder(z + sg(\hat{z}-z))||^2_2$$</p><p>这样在前向计算时，使用$\hat{z}$而在反向传播时使用$z$。从而可以对encoder进行优化。</p><p>代码实现</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Encoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, pic_channels=<span class="hljs-number">1</span></span>):</span>        <span class="hljs-built_in">super</span>(Encoder, self).__init__()        self.conv1 = nn.Conv2d(in_channels=pic_channels, out_channels=latent_dims//<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">4</span>)        self.conv2 = nn.Conv2d(in_channels=latent_dims//<span class="hljs-number">2</span>, out_channels=latent_dims, kernel_size=<span class="hljs-number">4</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        x = self.conv1(x)        x = F.relu(x)        x = self.conv2(x)        <span class="hljs-comment">#print(x)</span>        <span class="hljs-keyword">return</span> x      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, pic_channels=<span class="hljs-number">1</span></span>):</span>        <span class="hljs-built_in">super</span>(Decoder, self).__init__()        self.conv_trans1 = nn.ConvTranspose2d(          in_channels=latent_dims, out_channels=latent_dims//<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">4</span>)        self.conv_trans2 = nn.ConvTranspose2d(          in_channels=latent_dims//<span class="hljs-number">2</span>, out_channels=pic_channels, kernel_size=<span class="hljs-number">4</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        x = self.conv_trans1(x)        x = F.relu(x)        x = self.conv_trans2(x)        <span class="hljs-keyword">return</span> x       <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VectorQuantizer</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, num_codes=<span class="hljs-number">32</span>, beta=<span class="hljs-number">0.25</span></span>):</span>        <span class="hljs-built_in">super</span>(VectorQuantizer, self).__init__()        self.K = num_codes        self.D = latent_dims        self.beta = beta        self.codebook = nn.Embedding(self.K, self.D)        self.codebook.weight.data.uniform_(<span class="hljs-number">-1</span> / self.K, <span class="hljs-number">1</span> / self.K)        self.vq_loss = <span class="hljs-number">0</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, latents</span>):</span>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">         latents: (batch, dim, height, width)</span><span class="hljs-string">         codebook: (K, dim)</span><span class="hljs-string">        &#x27;&#x27;&#x27;</span>        <span class="hljs-comment"># convert latents from BCHW -&gt; BHWC</span>        latents = latents.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>).contiguous() <span class="hljs-comment"># (B, H, W, dim)</span>        latents_shape = latents.shape                <span class="hljs-comment"># Flatten latent</span>        flat_latent = latents.view(<span class="hljs-number">-1</span>, self.D) <span class="hljs-comment"># (BHW, dim)</span>        <span class="hljs-comment"># Compute L2 distance between latents and codes in codebook</span>        dist = (flat_latent.unsqueeze(<span class="hljs-number">1</span>) - self.codebook.weight.unsqueeze(<span class="hljs-number">0</span>)) ** <span class="hljs-number">2</span> <span class="hljs-comment"># (BHW, 1, dim) - (1, K, dim) -&gt; (BHW, K, dim)</span>        dist = dist.<span class="hljs-built_in">sum</span>(<span class="hljs-number">-1</span>) <span class="hljs-comment"># (BHW, K)</span>        <span class="hljs-comment"># Get the code index that has the min distance</span>        nearest_idxs = torch.argmin(dist, dim=<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># (BHW, 1)</span>        <span class="hljs-comment"># Convert to one-hot</span>        nearest_one_hot = torch.zeros(nearest_idxs.size(<span class="hljs-number">0</span>), self.K, device=latents.device) <span class="hljs-comment"># (BHW, K)</span>        nearest_one_hot.scatter_(<span class="hljs-number">1</span>, nearest_idxs, <span class="hljs-number">1</span>)  <span class="hljs-comment"># .scatter(dim,index,src)</span>        <span class="hljs-comment"># Quantize the latents</span>        quantized_latents = torch.matmul(nearest_one_hot, self.codebook.weight).view(latents_shape) <span class="hljs-comment"># (BHW, K) * (K, dim) = (BHW, dim) -&gt; (B, H, W, dim)</span>        <span class="hljs-comment"># Compute the VQ Losses</span>        commitment_loss = F.mse_loss(quantized_latents.detach(), latents)        codebook_loss = F.mse_loss(quantized_latents, latents.detach())        self.vq_loss = commitment_loss * self.beta + codebook_loss        <span class="hljs-comment"># convert quantized from BHWC -&gt; BCHW</span>        quantized_latents = latents + (quantized_latents - latents).detach()<span class="hljs-keyword">return</span> quantized_latents.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous()      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VQVariationalAutoencoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, ema=<span class="hljs-literal">True</span></span>):</span>        <span class="hljs-built_in">super</span>(VQVariationalAutoencoder, self).__init__()        self.encoder = Encoder(latent_dims)        <span class="hljs-keyword">if</span> ema:          self.vector_quantizer = VectorQuantizerEMA(latent_dims)        <span class="hljs-keyword">else</span>:          self.vector_quantizer = VectorQuantizer(latent_dims)        self.decoder = Decoder(latent_dims)        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        z_e = self.encoder(x)        z_q = self.vector_quantizer(z_e) <span class="hljs-comment"># (batch, dim, 22, 22)</span>        <span class="hljs-keyword">return</span> self.decoder(z_q)</code></pre><p>其中在74行可以看出我们VQVAE设计的梯度。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>vision transformer</title>
    <link href="/2021/07/06/transformer/"/>
    <url>/2021/07/06/transformer/</url>
    
    <content type="html"><![CDATA[<h1 id="本文记录一下我看过的transformer文章，"><a href="#本文记录一下我看过的transformer文章，" class="headerlink" title="本文记录一下我看过的transformer文章，"></a>本文记录一下我看过的transformer文章，</h1>]]></content>
    
    
    
    <tags>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>fast-slow-pointer</title>
    <link href="/2020/11/08/fast-slow-pointer/"/>
    <url>/2020/11/08/fast-slow-pointer/</url>
    
    <content type="html"><![CDATA[<p>快慢指针的用法，及其应用场景。</p>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用github搭建自己的博客</title>
    <link href="/2020/11/08/hello-world/"/>
    <url>/2020/11/08/hello-world/</url>
    
    <content type="html"><![CDATA[<p>下面记录一下我是如何利用GitHub来搭建自己的博客。</p><h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2><ul><li>安装Node.js</li></ul><p>官网为：<a href="https://nodejs.org/zh-cn/">https://nodejs.org/zh-cn/</a> 建议下载安装长期支持版。安装后win+r打开终端，测试npm命令行工具能不能用</p><pre><code class="hljs coffeescript"><span class="hljs-built_in">npm</span> -v</code></pre><p>注：若已安装过Node.js，可以查看一下版本号，确保和下面要安装的Hexo兼容</p><pre><code class="hljs crmsh"><span class="hljs-comment"># 查看Node.js版本</span><span class="hljs-keyword">node</span> <span class="hljs-title">-v</span></code></pre><p><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/20210630162300.png" alt="20210630162300"></p><ul><li>安装git</li></ul><p>这个相信大家应该都有，没有的话去 <a href="https://git-scm.com/download/win">https://git-scm.com/download/win</a> 下载安装</p><ul><li>安装Hexo</li></ul><p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p><pre><code class="hljs avrasm"><span class="hljs-meta"># 安装hexo</span>npm install -g hexo-<span class="hljs-keyword">cli</span></code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
