<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>DALL-E</title>
    <link href="/2022/03/31/DALL-E/"/>
    <url>/2022/03/31/DALL-E/</url>
    
    <content type="html"><![CDATA[<h2 id="dall-e">DALL-E</h2><p>主要由三部分组成，dVAE、transformer、CLIP。</p><p>120亿参数，250m的text-iamge对数据。</p><p>两阶段训练过程：</p><ul><li>训练一个dVAE，作用是将原本<span class="math inline">\(256 \times 256 \times 3\)</span>的RGB图片降为<span class="math inline">\(32 \times 32\)</span>的codebook，每个取值范围为<span class="math inline">\([0, 8192]\)</span>的整数。相当于整体降维了192。避免后续训练transformer出现大内存、计算消耗。</li><li>将一阶段得到的图片的<span class="math inline">\(32 \times 32 = 1024\)</span>token和text的256个token进行联合训练一个transformer。</li></ul><h3 id="dvae">dVAE</h3><p>离散分布的重参数：Gumbel Softmax</p><p><strong>重参数</strong>主要为了处理求期望形式的目标函数，对应于ELBO中的第一项。</p><p><span class="math display">\[L_\theta = E_{z-p_\theta(z)}[f(z)]\]</span></p><p>对应于dVAE中的离散情况，我们可以将其写作：</p><p><span class="math display">\[\sum_z p_\theta(z)f(z)\]</span></p><p>直观上来看，z是离散值，有有限种值，那么可以直接对其进行求和求解。但是通常来讲，z的取值范围过大，这是一个几乎没有可能接受的做法。因此采用采样的方式来对其进行估计，直接进行采样，会丢失梯度，无法进行反传，采用Gumbel Softmax方法。</p><h3 id="transformer">transformer</h3><p>text: 256 token size 16384</p><p>image: 1024 token size 8192</p><h3 id="clip">CLIP</h3><p>rerank结果，挑选topk</p><h2 id="rq-vae">RQ-VAE</h2><h3 id="vq-vae">VQ-VAE</h3><p>codebook C: <span class="math inline">\(\{ (k, e(k)\}_{k \in [K]}\)</span>, K表示codebook的大小, <span class="math inline">\(e(k) \in R^{n_z}\)</span></p><p>输入图片：X, encoder：E, decoder: G,</p><p>给定一个vector <span class="math inline">\(z \in R^{n_z}\)</span>, 使用<span class="math inline">\(Q(z; C)\)</span>来表示<span class="math inline">\(z\)</span>的<span class="math inline">\(VQ\)</span>，其计算也十分简单，为：</p><p><span class="math display">\[ Q(z; C) = argmin_{k \in [K]} ||z-e(k)||_2^2\]</span></p><p>那么对于整张图片<span class="math inline">\(X \in R^{H_o \times W_o \times 3}\)</span>，其在输入编码器后得到</p><p><span class="math display">\[Z = E(X) \in R^{H \times W \times n_z}\]</span></p><p>这里我们使用VQ对Z进行编码，得到</p><p><span class="math display">\[M_{hw} = Q(Z_{HW}; C), M_{hw} \in [K]^{hw}\]</span></p><p>即使用codebook的index来表示图片。之后就可以使用 quantized 特征<span class="math inline">\(\hat{Z}\)</span>来代替原始的feature map Z:</p><p><span class="math display">\[\hat{Z} = e(M_{hw}) \in R^{H \times W \times n_z}\]</span></p><p>之后即可使用<span class="math inline">\(\hat{z}\)</span> 送入G中对X进行重建</p><p><span class="math display">\[\hat{X} = G(\hat{Z}) \in R^{H_o \times W_o \times 3}\]</span></p><p>其中有几个值的大小需要重要理解，HW的大小表示将一个图片encoder后的code大小，其影响后续任务的复杂度，HW越小，复杂度越小。K表示整个codebook的大小，即图片能够选择的离散vector的多少，其越大，我们可以选择的特征越多，那么在计算<span class="math inline">\(Q(z; C)\)</span>时能得到更好的特征近似。</p><p>根据信息失真理论，我们使用<span class="math inline">\(HWlog_2 K\)</span> bits的数据去表示一张图片。</p><h3 id="rq-vae-1">RQ-VAE</h3><p>对于DALL-E来说，使用dVAE将一张<span class="math inline">\(256 \times 256\)</span>的图片重新编码为<span class="math inline">\(32 \times 32\)</span>的一系列分离的vector。而对于后续任务来说，更少的code有利于减少计算复杂度和空间复杂度，因为其和HW的二次成正比。那么减少编码后的HW看起来是一个降低复杂度的很理想的答案。但是如果保持整个codebook的大小不变的话，更少的code意味着会带来更大的重建损失，即code所表示的特征与原图片特征图差别会更大。然而我们如果采用增大codebook的方式来减少这种重建损失，又会带来 <strong>codebook collapse problem</strong>(此处还没有去深入了解)。因此本文提出使用Residual Quantization来解决这件事情。</p><p><strong>解决方法：</strong></p><p>采用循环生成策略，每个特征对应的不再是一个code，而是一个code序列。</p><p>不管是增大HW还是增大K，我们的目标都是在向减少重建误差靠齐。那么我们是否有办法使得codebook中的<span class="math inline">\(e(M)\)</span>vector逼近图像的feature map呢。</p><p>RQ-VAE给出了一个多次逼近的想法，即对于VQ-VAE，我们是在codebook中寻找一个最接近图片feature的vector作为其quantized 特征。而RQ-VAE在寻找到这个特征之后，会计算这个特征与feature的差值，然后在codebook中再寻找一个vector作为差值的近似，按照这种做法进行D次，得到一个深度为D的code。表示如下</p><p><span class="math display">\[RQ(z;C, D) = (k_1, ..., K_D) \in [K]^{D}\]</span></p><p>我们将一个<span class="math inline">\(z \in R^{n_z}\)</span>用D个index表示，其中每个index <span class="math inline">\(k\)</span> 计算如下</p><p><span class="math display">\[k_d = Q(r_{d-1};C)\]</span></p><p><span class="math display">\[r_d = r_{d-1}-e(k_d)\]</span></p><p>我们定义<span class="math inline">\(r_0 = z\)</span>, <span class="math inline">\(\hat{z}^{(d)} = \sum_{i=1}^{d} e(k_i)\)</span> ,那么<span class="math inline">\(\hat{z}^1\)</span>就相当于VQ中的<span class="math inline">\(\hat{z}\)</span>。这种思想的做法非常简单，就是通过多次的逼近误差，来达到更精确逼近<span class="math inline">\(z\)</span>的想法。这种做法不会在没有增加K和HW的情况下，达到了更精确逼近特征的目的。</p><p>对于一个图片<span class="math inline">\(X \in R^{H_o \times W_o \times 3}\)</span>，我们将其送入RQ-VAE，其会得到一个编码后的code <span class="math inline">\(M \in [K]^{H \times W \times D}\)</span>，相比于VQ-VAE多出了一个深度D维，但是这并不影响之后的解码使用到的特征维数，因为这个深度D维所表示的是各阶误差，在使用时并不单独使用。</p><p><span class="math display">\[M_{hw} = RQ(E(X)_{hw}; C, D)\]</span></p><p><span class="math display">\[\hat{Z}^{(d)}_{hw} = \sum_{d&#39;=1}^d e(M_{hwd&#39;})\]</span></p><p>在进行解码时，我们是使用<span class="math inline">\(\hat{Z}^{(D)}\)</span>来作为解码器<span class="math inline">\(G\)</span>的输入，这里<span class="math inline">\(\hat{Z}^{(D)}\)</span>可以理解为拥有最精确估计。</p><p><span class="math display">\[\hat{X} = G(\hat{Z}^{(D)})\]</span></p><p><strong>训练LOSS</strong></p><p>RQ-VAE的loss有两部分组成，一个是图片重建误差loss，另一个是各阶<span class="math inline">\(\hat{Z}^{(D)}\)</span>的commitment loss</p><p><span class="math display">\[L = L_{recon} + \beta L_{commit}\]</span></p><p><span class="math display">\[L_{recon} = ||X-\hat{X}||^2_2\]</span></p><p><span class="math display">\[L_{commit} = \sum_{d=1}^D ||Z-sg(\hat{Z}^{(d)})||^2_2\]</span></p><p>其中<span class="math inline">\(sg(\cdot)\)</span>表示禁止梯度反传。</p><h3 id="rq-transformer">RQ-transformer</h3><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/image-20220330185756506.png" alt="" /><figcaption>image-20220330185756506</figcaption></figure><p>分为空间transformer和深度D的transformer两部分</p><p>空间tansformer将<span class="math inline">\(\hat{Z}^{(d-1)}\)</span>作为输入，来预测下一个值，第一个embedding使用可训练的embedding，并在不同的子任务中使用方式不同，例如在text-condition的任务中，使用text embedding作为头。</p><p><span class="math display">\[u_t = PE_T(t) + \sum_{d=1}^D e(S_{t-1, d}) \, \, for \, t &gt; 1\]</span></p><p>而深度D的transformer略显复杂一些，其输入v相当于各阶层不同精确度的<span class="math inline">\(\hat{Z}^{(d)}\)</span>，而且需要对不同的时间都做。</p><p><span class="math display">\[v_{td} = PE_D(d)+\sum_{d&#39;=1}^{d-1} e(S_{td&#39;}) \, \, for \, d&gt;1\]</span></p><p>和空间基本类似，但特殊的，其第一个输入为当前t的空间transformer输出，即可以把深度D看为一个循环预测的transformer。<span class="math inline">\(d=1, v_{t1} = PE_D(1) + h_t\)</span>.</p><h2 id="clip-gen">CLIP-GEN</h2><p>整体来说网络没什么创新点，感觉就是CLIP VQVAE GPT的组合。主要创新点在于使用CLIP来间接的获取text-image 对，从而实现了使用没有label的数据进行训练的想法。</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/image-20220330204854561.png" alt="" /><figcaption>image-20220330204854561</figcaption></figure><ol type="a"><li><p>使用无标签数据预训练一个VQVAE，将图片token化</p></li><li><p>使用CLIP将图片数据映射到一个和text相同的特征空间，利用a预训练好的VQVAE，将图片token化;之后训练一个从embedding到token的自回归transformer，来完成从CLIP特征空间到VQVAE token的映射，从而可以使用VQVAE的decoder完成图片生成</p></li><li><p>测试过程，使用CLIP将text映射到统一特征空间，使用b训练的transformer将embedding转化为token,然后使用decoder生成图片。</p></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>VAE</title>
    <link href="/2022/03/31/VAE/"/>
    <url>/2022/03/31/VAE/</url>
    
    <content type="html"><![CDATA[<h2 id="vae">VAE</h2><p>本篇从VAE开始讲起，主要来分析VAE的作用，限制，以及后来的一些改进模型。主要参考 <a href="https://kexue.fm/">科学空间</a>、<a href="https://panxiaoxie.cn">panxiaoxie</a>.</p><h3 id="ae">AE</h3><p>Auto-Encoder 模型是一种非常常见的学习高维数据的低维表示的方式，我们通常训练一个Auto-Encoder模型来让其自动的学习图像的低维表示。</p><p>但是其只是将一幅图像映射到了隐空间中的一个向量上（一个点），不同的图像映射到不同的隐空间中的点。（这个与VQ-VAE还是不一样，虽然大家都是离散的向量）。其并不具备生成没见过的图像的性质，即如果在隐空间中随便找一个vector，然后使用decoder去解码，大概率是得不到想要的图像的。</p><p>这里<strong>不能进行生成的一个解释</strong>是：</p><p>对于AE模型，由于其encoder和decoder都是神经网络，使用了非线性变化的过程，其隐空间变量和原始空间之间可能并不能找到一个合理的关系，导致随机采样得到的解码结果往往是乱码或十分模糊。</p><h3 id="vae-1">VAE</h3><p>上面说到，AE模型中的每个样本，都会映射到隐空间的一个点上，这导致其在隐空间中随机采样一个点并不能得到好的生成效果。VAE的想法与此不同，其不再将每个样本映射到隐空间的一个点上，而是将每一个样本都映射到一个分布上去，然后再从这个分布上采样一个点，去重建原本的样本，这样就做到了可以在隐空间中采样去生成图像。</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/2584918486.png" alt="" /><figcaption>事实上，vae是为每个样本构造专属的正态分布，然后采样来重构</figcaption></figure><p>图像来源<a href="https://kexue.fm/archives/5253">科学空间</a></p><p>生成模型的目标在于找到数据样本的分布，即我们给定一个数据集<span class="math inline">\(X = \{X_1, X_2, ... X_n\}\)</span>, 如果我们能找到一个分布<span class="math inline">\(p(X)\)</span>, 其表示了数据集<span class="math inline">\(X\)</span>的分布，那么我们就可以直接在<span class="math inline">\(p(X)\)</span>中采样，得到所有的数据，包括数据集<span class="math inline">\(X\)</span>中没有出现过的数据。然而这个目标是无法实现的，我们可以通过另一种方式来实现：</p><p><span class="math display">\[p(X) = \sum_m p(m)p(x|m) = \int_zp(z)p(x|z)dz\]</span></p><p>上面，<span class="math inline">\(m \sim p(m), x|m \sim N(\mu^m, \delta^m)\)</span>。经过这样的变化，我们将存在大量失真区域的隐空间，转变为连续的隐空间。这里，VAE的想法就非常自然了，既然AE模型得到的只是隐空间中的一个点，那么VAE直接将其映射到一个正态分布，使得其能够包含整个隐空间。</p><p>我们可以看出来，整个模型中包含：</p><p><span class="math display">\[p(z) \sim N(0, 1), \]</span>先验概率，即假设噪声z的分布服从标准正态分布。</p><p><span class="math display">\[p(x|z)\]</span>， 似然概率，其对应于VAE中的decoder模型</p><p><span class="math display">\[q(z|x)\]</span>，后验分布，对应于VAE中的encoder模型，假设也为正态分布。</p><p>我们从上可以看出，因为我们已经假设p(z)，那么我们只要最大似然就可以很容易的优化<span class="math display">\[p(X)\]</span>，然而由于我们不可能去采样所有的<span class="math inline">\(p(z)\)</span>，因此这种方法不可行。</p><p>那么我们使用<span class="math inline">\(q(z|x)\)</span>来辅助求解<span class="math inline">\(p(x|z)\)</span>。我们的目标在于最大化<span class="math inline">\(log \,p(X)\)</span></p><p>我们可以推导：</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/VAE.png" alt="" /><figcaption>[公式]</figcaption></figure><p>右边一项由于KL函数的性质，恒大于0，因此我们找到了<span class="math inline">\(log \, P(x)\)</span>的一个下界：</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/ELBO.png" alt="" /><figcaption>[公式]</figcaption></figure><p>我们将其记做<span class="math inline">\(L_b\)</span>，那么原式可以写作：</p><p><span class="math display">\[logP(x) = L_b+KL(q(z|x)||P(z|x))\]</span></p><p>根据公式，<span class="math inline">\(P(x) = \int_zP(z)P(x|z)dz\)</span>，当我们固定了<span class="math inline">\(P(x|z)\)</span>，那么<span class="math inline">\(P(x)\)</span>就是固定的，而此时我们可以通过调整<span class="math inline">\(q(z|x)\)</span>即编码器，来使得KL散度项趋近于零。也就是说，L_b项可以代表我们想要求的<span class="math inline">\(logP\)</span>。</p><p>那么我们的优化目标就可以变为优化<span class="math inline">\(L_b\)</span>。</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/ELBO2.png" alt="" /><figcaption>[公式]</figcaption></figure><p>对于第一项，由于我们有假设<span class="math inline">\(q \sim N(\mu, \delta^2), P(z) \sim N(0, 1)\)</span>，可以根据KL散度公式进行展开，推导过程借用：</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/KL.png" alt="" /><figcaption>img</figcaption></figure><p>对于第二项，我们可以将其表示为</p><p><span class="math display">\[\int_zq(z|x)logP(x|z)dz = E_{q(z|x)}[logP(x|z)]\]</span></p><p>可以将其理解为AE的损失函数。</p><p>实际上我们从另一个角度理解这个损失函数，可以更具体一些。即我们一个朴素的思想还是，使得我们的重构误差最小，即上式中的第二项。但是VAE为什么还要有其他的loss呢？直观的，我们如果只包含一个重构loss，那么由于encoder得出的是一个均值和方差，为了让loss更小，模型学习的方法肯定会让方差等于0，也就是失去了随机噪声，VAE就会退化为AE模型。那么我们另一个直观的想法，既然后验分布需要一定的噪声，那么我们就强迫其向标准正态靠近，因此可以加一个loss来衡量它和标准正态的距离，这个衡量函数就是KL散度。</p><p>代码实现：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VariationalEncoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims</span>):</span>        <span class="hljs-built_in">super</span>(VariationalEncoder, self).__init__()        self.linear1 = nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">512</span>)        self.linear2 = nn.Linear(<span class="hljs-number">512</span>, latent_dims)        self.linear3 = nn.Linear(<span class="hljs-number">512</span>, latent_dims)                self.kl = <span class="hljs-number">0</span>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        x = torch.flatten(x, start_dim=<span class="hljs-number">1</span>)        x = F.relu(self.linear1(x))        mu =  self.linear2(x)        sigma = torch.exp(self.linear3(x))                z = mu + sigma*torch.randn_like(sigma)        self.kl = <span class="hljs-number">0.5</span>*(sigma**<span class="hljs-number">2</span> + mu**<span class="hljs-number">2</span> - torch.log(sigma) - <span class="hljs-number">1</span>).<span class="hljs-built_in">sum</span>()        <span class="hljs-keyword">return</span> z<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims</span>):</span>        <span class="hljs-built_in">super</span>(Decoder, self).__init__()        self.linear1 = nn.Linear(latent_dims, <span class="hljs-number">512</span>)        self.linear2 = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">784</span>)            <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, z</span>):</span>        x_hat = F.relu(self.linear1(z))        x_hat = torch.sigmoid(self.linear2(x_hat))        <span class="hljs-keyword">return</span> x_hat.reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VariationalAutoencoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims</span>):</span>        <span class="hljs-built_in">super</span>(VariationalAutoencoder, self).__init__()        self.encoder = VariationalEncoder(latent_dims)        self.decoder = Decoder(latent_dims)        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        z = self.encoder(x)        <span class="hljs-keyword">return</span> self.decoder(z)</code></pre><h3 id="vqvae">VQVAE</h3><p>不同于VAE，其不再将图片编码到一个连续的空间，而是像AE一样的离散空间。其过程为</p><p>codebook C: <span class="math inline">\(\{ (k, e(k)\}_{k \in [K]}\)</span>, K表示codebook的大小, <span class="math inline">\(e(k) \in R^{n_z}\)</span></p><p>输入图片：X, encoder：E, decoder: G,</p><p>给定一个vector <span class="math inline">\(z \in R^{n_z}\)</span>, 使用<span class="math inline">\(Q(z; C)\)</span>来表示<span class="math inline">\(z\)</span>的<span class="math inline">\(VQ\)</span>，其计算也十分简单，为：</p><p><span class="math display">\[ Q(z; C) = argmin_{k \in [K]} ||z-e(k)||_2^2\]</span></p><p>那么对于整张图片<span class="math inline">\(X \in R^{H_o \times W_o \times 3}\)</span>，其在输入编码器后得到</p><p><span class="math display">\[Z = E(X) \in R^{H \times W \times n_z}\]</span></p><p>这里我们使用VQ对Z进行编码，得到</p><p><span class="math display">\[M_{hw} = Q(Z_{HW}; C), M_{hw} \in [K]^{hw}\]</span></p><p>即使用codebook的index来表示图片。之后就可以使用 quantized 特征<span class="math inline">\(\hat{Z}\)</span>来代替原始的feature map Z:</p><p><span class="math display">\[\hat{Z} = e(M_{hw}) \in R^{H \times W \times n_z}\]</span></p><p>之后即可使用<span class="math inline">\(\hat{z}\)</span> 送入G中对X进行重建</p><p><span class="math display">\[\hat{X} = G(\hat{Z}) \in R^{H_o \times W_o \times 3}\]</span></p><p><strong>梯度反传</strong></p><p>我们可以看到，在前向求<span class="math inline">\(\hat{Z}\)</span>时，用到了<span class="math inline">\(arg min\)</span>操作，这个操作本身是没有梯度的，如果我们在优化时，使用着一个loss：</p><p><span class="math display">\[||x-decoder(\hat{z})||^2_2\]</span></p><p>其梯度不会更新encoder，梯度在<span class="math inline">\(argmin\)</span>那里就停止了。这里VQVAE使用了Straight-Through Estimator方法，其思想十分简单，就是在前向传播时使用自己想要得变量进行计算，而在反向chanson时，使用自己为其设计的梯度。在这里，VQVAE就是用了<span class="math inline">\(z\)</span>的梯度来代替<span class="math inline">\(\hat{z}\)</span>：</p><p><span class="math display">\[||x-decoder(z + sg(\hat{z}-z))||^2_2\]</span></p><p>这样在前向计算时，使用<span class="math inline">\(\hat{z}\)</span>而在反向传播时使用<span class="math inline">\(z\)</span>。从而可以对encoder进行优化。</p><p>代码实现</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Encoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, pic_channels=<span class="hljs-number">1</span></span>):</span>        <span class="hljs-built_in">super</span>(Encoder, self).__init__()        self.conv1 = nn.Conv2d(in_channels=pic_channels, out_channels=latent_dims//<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">4</span>)        self.conv2 = nn.Conv2d(in_channels=latent_dims//<span class="hljs-number">2</span>, out_channels=latent_dims, kernel_size=<span class="hljs-number">4</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        x = self.conv1(x)        x = F.relu(x)        x = self.conv2(x)        <span class="hljs-comment">#print(x)</span>        <span class="hljs-keyword">return</span> x      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, pic_channels=<span class="hljs-number">1</span></span>):</span>        <span class="hljs-built_in">super</span>(Decoder, self).__init__()        self.conv_trans1 = nn.ConvTranspose2d(          in_channels=latent_dims, out_channels=latent_dims//<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">4</span>)        self.conv_trans2 = nn.ConvTranspose2d(          in_channels=latent_dims//<span class="hljs-number">2</span>, out_channels=pic_channels, kernel_size=<span class="hljs-number">4</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        x = self.conv_trans1(x)        x = F.relu(x)        x = self.conv_trans2(x)        <span class="hljs-keyword">return</span> x       <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VectorQuantizer</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, num_codes=<span class="hljs-number">32</span>, beta=<span class="hljs-number">0.25</span></span>):</span>        <span class="hljs-built_in">super</span>(VectorQuantizer, self).__init__()        self.K = num_codes        self.D = latent_dims        self.beta = beta        self.codebook = nn.Embedding(self.K, self.D)        self.codebook.weight.data.uniform_(<span class="hljs-number">-1</span> / self.K, <span class="hljs-number">1</span> / self.K)        self.vq_loss = <span class="hljs-number">0</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, latents</span>):</span>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">         latents: (batch, dim, height, width)</span><span class="hljs-string">         codebook: (K, dim)</span><span class="hljs-string">        &#x27;&#x27;&#x27;</span>        <span class="hljs-comment"># convert latents from BCHW -&gt; BHWC</span>        latents = latents.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>).contiguous() <span class="hljs-comment"># (B, H, W, dim)</span>        latents_shape = latents.shape                <span class="hljs-comment"># Flatten latent</span>        flat_latent = latents.view(<span class="hljs-number">-1</span>, self.D) <span class="hljs-comment"># (BHW, dim)</span>        <span class="hljs-comment"># Compute L2 distance between latents and codes in codebook</span>        dist = (flat_latent.unsqueeze(<span class="hljs-number">1</span>) - self.codebook.weight.unsqueeze(<span class="hljs-number">0</span>)) ** <span class="hljs-number">2</span> <span class="hljs-comment"># (BHW, 1, dim) - (1, K, dim) -&gt; (BHW, K, dim)</span>        dist = dist.<span class="hljs-built_in">sum</span>(<span class="hljs-number">-1</span>) <span class="hljs-comment"># (BHW, K)</span>        <span class="hljs-comment"># Get the code index that has the min distance</span>        nearest_idxs = torch.argmin(dist, dim=<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># (BHW, 1)</span>        <span class="hljs-comment"># Convert to one-hot</span>        nearest_one_hot = torch.zeros(nearest_idxs.size(<span class="hljs-number">0</span>), self.K, device=latents.device) <span class="hljs-comment"># (BHW, K)</span>        nearest_one_hot.scatter_(<span class="hljs-number">1</span>, nearest_idxs, <span class="hljs-number">1</span>)  <span class="hljs-comment"># .scatter(dim,index,src)</span>        <span class="hljs-comment"># Quantize the latents</span>        quantized_latents = torch.matmul(nearest_one_hot, self.codebook.weight).view(latents_shape) <span class="hljs-comment"># (BHW, K) * (K, dim) = (BHW, dim) -&gt; (B, H, W, dim)</span>        <span class="hljs-comment"># Compute the VQ Losses</span>        commitment_loss = F.mse_loss(quantized_latents.detach(), latents)        codebook_loss = F.mse_loss(quantized_latents, latents.detach())        self.vq_loss = commitment_loss * self.beta + codebook_loss        <span class="hljs-comment"># convert quantized from BHWC -&gt; BCHW</span>        quantized_latents = latents + (quantized_latents - latents).detach()<span class="hljs-keyword">return</span> quantized_latents.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous()      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VQVariationalAutoencoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, ema=<span class="hljs-literal">True</span></span>):</span>        <span class="hljs-built_in">super</span>(VQVariationalAutoencoder, self).__init__()        self.encoder = Encoder(latent_dims)        <span class="hljs-keyword">if</span> ema:          self.vector_quantizer = VectorQuantizerEMA(latent_dims)        <span class="hljs-keyword">else</span>:          self.vector_quantizer = VectorQuantizer(latent_dims)        self.decoder = Decoder(latent_dims)        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        z_e = self.encoder(x)        z_q = self.vector_quantizer(z_e) <span class="hljs-comment"># (batch, dim, 22, 22)</span>        <span class="hljs-keyword">return</span> self.decoder(z_q)</code></pre><p>其中在74行可以看出我们VQVAE设计的梯度。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>学习记录</title>
    <link href="/2021/07/06/transformer/"/>
    <url>/2021/07/06/transformer/</url>
    
    <content type="html"><![CDATA[<h3 id="warm-up">warm up</h3><p>warm up 是一种治标之法，warm up能解决模型初期难以收敛的问题，但这种解决方式并不根本。因为模型前期难以收敛意味着模型会受到较大的扰动，即一点梯度的变化会影响整个模型的抖动。我们应该做的是修改模型的结构来解决这个问题而不是降低学习率。可以考虑是否是层数太多，一般来说，层数多的话，需要降低梯度的大小，否则会导致不收敛。</p><h3 id="增量爆炸">增量爆炸</h3><p>模型越深，对输出的扰动就越大。</p><h3 id="transformer-的pre-norm-和post-norm">transformer 的Pre Norm 和Post Norm</h3><p>一般来说，Pre Norm会比Post Norm更容易训练一些，但是Post Norm的最终效果往往更好。清华的一篇text-2-img文章CogView中，提出了一个三明治型的Norm方式，即同时使用Pre Norm 和 Post Norm。</p>]]></content>
    
    
    
    <tags>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>fast-slow-pointer</title>
    <link href="/2020/11/08/fast-slow-pointer/"/>
    <url>/2020/11/08/fast-slow-pointer/</url>
    
    <content type="html"><![CDATA[<p>快慢指针的用法，及其应用场景。</p>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用github搭建自己的博客</title>
    <link href="/2020/11/08/hello-world/"/>
    <url>/2020/11/08/hello-world/</url>
    
    <content type="html"><![CDATA[<p>下面记录一下我是如何利用GitHub来搭建自己的博客。</p><h2 id="准备工作">1. 准备工作</h2><ul><li>安装Node.js</li></ul><p>官网为：https://nodejs.org/zh-cn/ 建议下载安装长期支持版。安装后win+r打开终端，测试npm命令行工具能不能用 <pre><code class="hljs coffeescript"><span class="hljs-built_in">npm</span> -v</code></pre> 注：若已安装过Node.js，可以查看一下版本号，确保和下面要安装的Hexo兼容 <pre><code class="hljs crmsh"><span class="hljs-comment"># 查看Node.js版本</span><span class="hljs-keyword">node</span> <span class="hljs-title">-v</span></code></pre> <img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/20210630162300.png" alt="20210630162300" /></p><ul><li>安装git</li></ul><p>这个相信大家应该都有，没有的话去 https://git-scm.com/download/win 下载安装</p><ul><li>安装Hexo</li></ul><p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p><pre><code class="hljs avrasm"><span class="hljs-meta"># 安装hexo</span>npm install -g hexo-<span class="hljs-keyword">cli</span></code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
