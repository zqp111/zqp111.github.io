<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>DALL-E</title>
    <link href="/2022/03/31/DALL-E/"/>
    <url>/2022/03/31/DALL-E/</url>
    
    <content type="html"><![CDATA[<h2 id="dall-e">DALL-E</h2><p>?????????dVAE?transformer?CLIP?</p><p>120????250m?text-iamge????</p><p>????????</p><ul><li>????dVAE???????<span class="math inline">\(256 \times 256 \times 3\)</span>?RGB????<span class="math inline">\(32 \times 32\)</span>?codebook????????<span class="math inline">\([0, 8192]\)</span>????????????192???????transformer???????????</li><li>??????????<span class="math inline">\(32 \times 32 = 1024\)</span>token?text?256?token????????transformer?</li></ul><h3 id="dvae">dVAE</h3><p>?????????Gumbel Softmax</p><p><strong>???</strong>????????????????????ELBO??????</p><p><span class="math display">\[L_\theta = E_{z-p_\theta(z)}[f(z)]\]</span></p><p>???dVAE????????????????</p><p><span class="math display">\[\sum_z p_\theta(z)f(z)\]</span></p><p>??????z?????????????????????????????????z???????????????????????????????????????????????????????????????Gumbel Softmax???</p><h3 id="transformer">transformer</h3><p>text: 256 token size 16384</p><p>image: 1024 token size 8192</p><h3 id="clip">CLIP</h3><p>rerank?????topk</p><h2 id="rq-vae">RQ-VAE</h2><h3 id="vq-vae">VQ-VAE</h3><p>codebook C: <span class="math inline">\(\{ (k, e(k)\}_{k \in [K]}\)</span>, K??codebook???, <span class="math inline">\(e(k) \in R^{n_z}\)</span></p><p>?????X, encoder?E, decoder: G,</p><p>????vector <span class="math inline">\(z \in R^{n_z}\)</span>, ??<span class="math inline">\(Q(z; C)\)</span>???<span class="math inline">\(z\)</span>?<span class="math inline">\(VQ\)</span>????????????</p><p><span class="math display">\[ Q(z; C) = argmin_{k \in [K]} ||z-e(k)||_2^2\]</span></p><p>????????<span class="math inline">\(X \in R^{H_o \times W_o \times 3}\)</span>???????????</p><p><span class="math display">\[Z = E(X) \in R^{H \times W \times n_z}\]</span></p><p>??????VQ?Z???????</p><p><span class="math display">\[M_{hw} = Q(Z_{HW}; C), M_{hw} \in [K]^{hw}\]</span></p><p>???codebook?index????????????? quantized ??<span class="math inline">\(\hat{Z}\)</span>??????feature map Z:</p><p><span class="math display">\[\hat{Z} = e(M_{hw}) \in R^{H \times W \times n_z}\]</span></p><p>??????<span class="math inline">\(\hat{z}\)</span> ??G??X????</p><p><span class="math display">\[\hat{X} = G(\hat{Z}) \in R^{H_o \times W_o \times 3}\]</span></p><p>????????????????HW??????????encoder??code???????????????HW?????????K????codebook??????????????vector?????????????????????????<span class="math inline">\(Q(z; C)\)</span>????????????</p><p>?????????????<span class="math inline">\(HWlog_2 K\)</span> bits???????????</p><h3 id="rq-vae-1">RQ-VAE</h3><p>??DALL-E?????dVAE???<span class="math inline">\(256 \times 256\)</span>????????<span class="math inline">\(32 \times 32\)</span>???????vector??????????????code?????????????????????HW???????????????HW???????????????????????????codebook???????????code???????????????code?????????????????????????????codebook????????????????? <strong>codebook collapse problem</strong>(??????????)?????????Residual Quantization????????</p><p><strong>?????</strong></p><p>?????????????????????code?????code???</p><p>?????HW????K??????????????????????????????codebook??<span class="math inline">\(e(M)\)</span>vector?????feature map??</p><p>RQ-VAE????????????????VQ-VAE?????codebook??????????feature?vector???quantized ????RQ-VAE???????????????????feature???????codebook??????vector????????????????D?????????D?code?????</p><p><span class="math display">\[RQ(z;C, D) = (k_1, ..., K_D) \in [K]^{D}\]</span></p><p>?????<span class="math inline">\(z \in R^{n_z}\)</span>?D?index???????index <span class="math inline">\(k\)</span> ????</p><p><span class="math display">\[k_d = Q(r_{d-1};C)\]</span></p><p><span class="math display">\[r_d = r_{d-1}-e(k_d)\]</span></p><p>????<span class="math inline">\(r_0 = z\)</span>, <span class="math inline">\(\hat{z}^{(d)} = \sum_{i=1}^{d} e(k_i)\)</span> ,??<span class="math inline">\(\hat{z}^1\)</span>????VQ??<span class="math inline">\(\hat{z}\)</span>?????????????????????????????????<span class="math inline">\(z\)</span>???????????????K?HW???????????????????</p><p>??????<span class="math inline">\(X \in R^{H_o \times W_o \times 3}\)</span>???????RQ-VAE???????????code <span class="math inline">\(M \in [K]^{H \times W \times D}\)</span>????VQ-VAE???????D?????????????????????????????D??????????????????????</p><p><span class="math display">\[M_{hw} = RQ(E(X)_{hw}; C, D)\]</span></p><p><span class="math display">\[\hat{Z}^{(d)}_{hw} = \sum_{d&#39;=1}^d e(M_{hwd&#39;})\]</span></p><p>????????????<span class="math inline">\(\hat{Z}^{(D)}\)</span>??????<span class="math inline">\(G\)</span>??????<span class="math inline">\(\hat{Z}^{(D)}\)</span>?????????????</p><p><span class="math display">\[\hat{X} = G(\hat{Z}^{(D)})\]</span></p><p><strong>??LOSS</strong></p><p>RQ-VAE?loss????????????????loss???????<span class="math inline">\(\hat{Z}^{(D)}\)</span>?commitment loss</p><p><span class="math display">\[L = L_{recon} + \beta L_{commit}\]</span></p><p><span class="math display">\[L_{recon} = ||X-\hat{X}||^2_2\]</span></p><p><span class="math display">\[L_{commit} = \sum_{d=1}^D ||Z-sg(\hat{Z}^{(d)})||^2_2\]</span></p><p>??<span class="math inline">\(sg(\cdot)\)</span>?????????</p><h3 id="rq-transformer">RQ-transformer</h3><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/image-20220330185756506.png" alt="" /><figcaption>image-20220330185756506</figcaption></figure><p>????transformer???D?transformer???</p><p>??tansformer?<span class="math inline">\(\hat{Z}^{(d-1)}\)</span>????????????????embedding??????embedding????????????????????text-condition???????text embedding????</p><p><span class="math display">\[u_t = PE_T(t) + \sum_{d=1}^D e(S_{t-1, d}) \, \, for \, t &gt; 1\]</span></p><p>???D?transformer??????????v????????????<span class="math inline">\(\hat{Z}^{(d)}\)</span>??????????????</p><p><span class="math display">\[v_{td} = PE_D(d)+\sum_{d&#39;=1}^{d-1} e(S_{td&#39;}) \, \, for \, d&gt;1\]</span></p><p>??????????????????????t???transformer?????????D?????????transformer?<span class="math inline">\(d=1, v_{t1} = PE_D(1) + h_t\)</span>.</p><h2 id="clip-gen">CLIP-GEN</h2><p>?????????????????CLIP VQVAE GPT?????????????CLIP??????text-image ???????????label???????????</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/image-20220330204854561.png" alt="" /><figcaption>image-20220330204854561</figcaption></figure><ol type="a"><li><p>????????????VQVAE????token?</p></li><li><p>??CLIP???????????text??????????a?????VQVAE????token?;???????embedding?token????transformer?????CLIP?????VQVAE token??????????VQVAE?decoder??????</p></li><li><p>???????CLIP?text????????????b???transformer?embedding???token,????decoder?????</p></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>VAE</title>
    <link href="/2022/03/31/VAE/"/>
    <url>/2022/03/31/VAE/</url>
    
    <content type="html"><![CDATA[<h2 id="vae">VAE</h2><p>???VAE??????????VAE??????????????????????? <a href="https://kexue.fm/">????</a>?<a href="https://panxiaoxie.cn">panxiaoxie</a>.</p><h3 id="ae">AE</h3><p>Auto-Encoder ?????????????????????????????????Auto-Encoder??????????????????</p><p>????????????????????????????????????????????????????VQ-VAE?????????????????????????????????????????????????vector?????decoder??????????????????</p><p>??<strong>???????????</strong>??</p><p>??AE??????encoder?decoder???????????????????????????????????????????????????????????????????????</p><h3 id="vae-1">VAE</h3><p>?????AE???????????????????????????????????????????????????VAE????????????????????????????????????????????????????????????????????????????????????????????</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/2584918486.png" alt="" /><figcaption>????vae???????????????????????</figcaption></figure><p>????<a href="https://kexue.fm/archives/5253">????</a></p><p>?????????????????????????????<span class="math inline">\(X = \{X_1, X_2, ... X_n\}\)</span>, ???????????<span class="math inline">\(p(X)\)</span>, ???????<span class="math inline">\(X\)</span>??????????????<span class="math inline">\(p(X)\)</span>?????????????????<span class="math inline">\(X\)</span>??????????????????????????????????????</p><p><span class="math display">\[p(X) = \sum_m p(m)p(x|m) = \int_zp(z)p(x|z)dz\]</span></p><p>???<span class="math inline">\(m \sim p(m), x|m \sim N(\mu^m, \delta^m)\)</span>??????????????????????????????????????VAE????????????AE??????????????????VAE???????????????????????????</p><p>????????????????</p><p><span class="math display">\[p(z) \sim N(0, 1), \]</span>??????????z????????????</p><p><span class="math display">\[p(x|z)\]</span>? ?????????VAE??decoder??</p><p><span class="math display">\[q(z|x)\]</span>?????????VAE??encoder????????????</p><p>?????????????????p(z)????????????????????<span class="math display">\[p(X)\]</span>????????????????<span class="math inline">\(p(z)\)</span>???????????</p><p>??????<span class="math inline">\(q(z|x)\)</span>?????<span class="math inline">\(p(x|z)\)</span>???????????<span class="math inline">\(log \,p(X)\)</span></p><p>???????</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/VAE.png" alt="" /><figcaption>[??]</figcaption></figure><p>??????KL?????????0????????<span class="math inline">\(log \, P(x)\)</span>??????</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/ELBO.png" alt="" /><figcaption>[??]</figcaption></figure><p>??????<span class="math inline">\(L_b\)</span>??????????</p><p><span class="math display">\[logP(x) = L_b+KL(q(z|x)||P(z|x))\]</span></p><p>?????<span class="math inline">\(P(x) = \int_zP(z)P(x|z)dz\)</span>???????<span class="math inline">\(P(x|z)\)</span>???<span class="math inline">\(P(x)\)</span>?????????????????<span class="math inline">\(q(z|x)\)</span>????????KL?????????????L_b???????????<span class="math inline">\(logP\)</span>?</p><p>????????????????<span class="math inline">\(L_b\)</span>?</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/ELBO2.png" alt="" /><figcaption>[??]</figcaption></figure><p>?????????????<span class="math inline">\(q \sim N(\mu, \delta^2), P(z) \sim N(0, 1)\)</span>?????KL????????????????</p><figure><img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/KL.png" alt="" /><figcaption>img</figcaption></figure><p>???????????????</p><p><span class="math display">\[\int_zq(z|x)logP(x|z)dz = E_{q(z|x)}[logP(x|z)]\]</span></p><p>???????AE??????</p><p>????????????????????????????????????????????????????????????????VAE?????????loss?????????????????loss?????encoder???????????????loss??????????????????0????????????VAE?????AE?????????????????????????????????????????????????????loss?????????????????????KL???</p><p>?????</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VariationalEncoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims</span>):</span>        <span class="hljs-built_in">super</span>(VariationalEncoder, self).__init__()        self.linear1 = nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">512</span>)        self.linear2 = nn.Linear(<span class="hljs-number">512</span>, latent_dims)        self.linear3 = nn.Linear(<span class="hljs-number">512</span>, latent_dims)                self.kl = <span class="hljs-number">0</span>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        x = torch.flatten(x, start_dim=<span class="hljs-number">1</span>)        x = F.relu(self.linear1(x))        mu =  self.linear2(x)        sigma = torch.exp(self.linear3(x))                z = mu + sigma*torch.randn_like(sigma)        self.kl = <span class="hljs-number">0.5</span>*(sigma**<span class="hljs-number">2</span> + mu**<span class="hljs-number">2</span> - torch.log(sigma) - <span class="hljs-number">1</span>).<span class="hljs-built_in">sum</span>()        <span class="hljs-keyword">return</span> z<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims</span>):</span>        <span class="hljs-built_in">super</span>(Decoder, self).__init__()        self.linear1 = nn.Linear(latent_dims, <span class="hljs-number">512</span>)        self.linear2 = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">784</span>)            <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, z</span>):</span>        x_hat = F.relu(self.linear1(z))        x_hat = torch.sigmoid(self.linear2(x_hat))        <span class="hljs-keyword">return</span> x_hat.reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VariationalAutoencoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims</span>):</span>        <span class="hljs-built_in">super</span>(VariationalAutoencoder, self).__init__()        self.encoder = VariationalEncoder(latent_dims)        self.decoder = Decoder(latent_dims)        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        z = self.encoder(x)        <span class="hljs-keyword">return</span> self.decoder(z)</code></pre><h3 id="vqvae">VQVAE</h3><p>???VAE?????????????????????AE????????????</p><p>codebook C: <span class="math inline">\(\{ (k, e(k)\}_{k \in [K]}\)</span>, K??codebook???, <span class="math inline">\(e(k) \in R^{n_z}\)</span></p><p>?????X, encoder?E, decoder: G,</p><p>????vector <span class="math inline">\(z \in R^{n_z}\)</span>, ??<span class="math inline">\(Q(z; C)\)</span>???<span class="math inline">\(z\)</span>?<span class="math inline">\(VQ\)</span>????????????</p><p><span class="math display">\[ Q(z; C) = argmin_{k \in [K]} ||z-e(k)||_2^2\]</span></p><p>????????<span class="math inline">\(X \in R^{H_o \times W_o \times 3}\)</span>???????????</p><p><span class="math display">\[Z = E(X) \in R^{H \times W \times n_z}\]</span></p><p>??????VQ?Z???????</p><p><span class="math display">\[M_{hw} = Q(Z_{HW}; C), M_{hw} \in [K]^{hw}\]</span></p><p>???codebook?index????????????? quantized ??<span class="math inline">\(\hat{Z}\)</span>??????feature map Z:</p><p><span class="math display">\[\hat{Z} = e(M_{hw}) \in R^{H \times W \times n_z}\]</span></p><p>??????<span class="math inline">\(\hat{z}\)</span> ??G??X????</p><p><span class="math display">\[\hat{X} = G(\hat{Z}) \in R^{H_o \times W_o \times 3}\]</span></p><p><strong>????</strong></p><p>???????????<span class="math inline">\(\hat{Z}\)</span>?????<span class="math inline">\(arg min\)</span>??????????????????????????????loss?</p><p><span class="math display">\[||x-decoder(\hat{z})||^2_2\]</span></p><p>???????encoder????<span class="math inline">\(argmin\)</span>?????????VQVAE???Straight-Through Estimator?????????????????????????????????????chanson??????????????????VQVAE????<span class="math inline">\(z\)</span>??????<span class="math inline">\(\hat{z}\)</span>?</p><p><span class="math display">\[||x-decoder(z + sg(\hat{z}-z))||^2_2\]</span></p><p>???????????<span class="math inline">\(\hat{z}\)</span>?????????<span class="math inline">\(z\)</span>??????encoder?????</p><p>????</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Encoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, pic_channels=<span class="hljs-number">1</span></span>):</span>        <span class="hljs-built_in">super</span>(Encoder, self).__init__()        self.conv1 = nn.Conv2d(in_channels=pic_channels, out_channels=latent_dims//<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">4</span>)        self.conv2 = nn.Conv2d(in_channels=latent_dims//<span class="hljs-number">2</span>, out_channels=latent_dims, kernel_size=<span class="hljs-number">4</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        x = self.conv1(x)        x = F.relu(x)        x = self.conv2(x)        <span class="hljs-comment">#print(x)</span>        <span class="hljs-keyword">return</span> x      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, pic_channels=<span class="hljs-number">1</span></span>):</span>        <span class="hljs-built_in">super</span>(Decoder, self).__init__()        self.conv_trans1 = nn.ConvTranspose2d(          in_channels=latent_dims, out_channels=latent_dims//<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">4</span>)        self.conv_trans2 = nn.ConvTranspose2d(          in_channels=latent_dims//<span class="hljs-number">2</span>, out_channels=pic_channels, kernel_size=<span class="hljs-number">4</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        x = self.conv_trans1(x)        x = F.relu(x)        x = self.conv_trans2(x)        <span class="hljs-keyword">return</span> x       <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VectorQuantizer</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, num_codes=<span class="hljs-number">32</span>, beta=<span class="hljs-number">0.25</span></span>):</span>        <span class="hljs-built_in">super</span>(VectorQuantizer, self).__init__()        self.K = num_codes        self.D = latent_dims        self.beta = beta        self.codebook = nn.Embedding(self.K, self.D)        self.codebook.weight.data.uniform_(<span class="hljs-number">-1</span> / self.K, <span class="hljs-number">1</span> / self.K)        self.vq_loss = <span class="hljs-number">0</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, latents</span>):</span>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">         latents: (batch, dim, height, width)</span><span class="hljs-string">         codebook: (K, dim)</span><span class="hljs-string">        &#x27;&#x27;&#x27;</span>        <span class="hljs-comment"># convert latents from BCHW -&gt; BHWC</span>        latents = latents.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>).contiguous() <span class="hljs-comment"># (B, H, W, dim)</span>        latents_shape = latents.shape                <span class="hljs-comment"># Flatten latent</span>        flat_latent = latents.view(<span class="hljs-number">-1</span>, self.D) <span class="hljs-comment"># (BHW, dim)</span>        <span class="hljs-comment"># Compute L2 distance between latents and codes in codebook</span>        dist = (flat_latent.unsqueeze(<span class="hljs-number">1</span>) - self.codebook.weight.unsqueeze(<span class="hljs-number">0</span>)) ** <span class="hljs-number">2</span> <span class="hljs-comment"># (BHW, 1, dim) - (1, K, dim) -&gt; (BHW, K, dim)</span>        dist = dist.<span class="hljs-built_in">sum</span>(<span class="hljs-number">-1</span>) <span class="hljs-comment"># (BHW, K)</span>        <span class="hljs-comment"># Get the code index that has the min distance</span>        nearest_idxs = torch.argmin(dist, dim=<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># (BHW, 1)</span>        <span class="hljs-comment"># Convert to one-hot</span>        nearest_one_hot = torch.zeros(nearest_idxs.size(<span class="hljs-number">0</span>), self.K, device=latents.device) <span class="hljs-comment"># (BHW, K)</span>        nearest_one_hot.scatter_(<span class="hljs-number">1</span>, nearest_idxs, <span class="hljs-number">1</span>)  <span class="hljs-comment"># .scatter(dim,index,src)</span>        <span class="hljs-comment"># Quantize the latents</span>        quantized_latents = torch.matmul(nearest_one_hot, self.codebook.weight).view(latents_shape) <span class="hljs-comment"># (BHW, K) * (K, dim) = (BHW, dim) -&gt; (B, H, W, dim)</span>        <span class="hljs-comment"># Compute the VQ Losses</span>        commitment_loss = F.mse_loss(quantized_latents.detach(), latents)        codebook_loss = F.mse_loss(quantized_latents, latents.detach())        self.vq_loss = commitment_loss * self.beta + codebook_loss        <span class="hljs-comment"># convert quantized from BHWC -&gt; BCHW</span>        quantized_latents = latents + (quantized_latents - latents).detach()<span class="hljs-keyword">return</span> quantized_latents.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous()      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VQVariationalAutoencoder</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, latent_dims, ema=<span class="hljs-literal">True</span></span>):</span>        <span class="hljs-built_in">super</span>(VQVariationalAutoencoder, self).__init__()        self.encoder = Encoder(latent_dims)        <span class="hljs-keyword">if</span> ema:          self.vector_quantizer = VectorQuantizerEMA(latent_dims)        <span class="hljs-keyword">else</span>:          self.vector_quantizer = VectorQuantizer(latent_dims)        self.decoder = Decoder(latent_dims)        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        z_e = self.encoder(x)        z_q = self.vector_quantizer(z_e) <span class="hljs-comment"># (batch, dim, 22, 22)</span>        <span class="hljs-keyword">return</span> self.decoder(z_q)</code></pre><p>???74???????VQVAE??????</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>学习记录</title>
    <link href="/2021/07/06/transformer/"/>
    <url>/2021/07/06/transformer/</url>
    
    <content type="html"><![CDATA[<h3 id="warm-up">warm up</h3><p>warm up 是一种治标之法，warm up能解决模型初期难以收敛的问题，但这种解决方式并不根本。因为模型前期难以收敛意味着模型会受到较大的扰动，即一点梯度的变化会影响整个模型的抖动。我们应该做的是修改模型的结构来解决这个问题而不是降低学习率。可以考虑是否是层数太多，一般来说，层数多的话，需要降低梯度的大小，否则会导致不收敛。</p><h3 id="增量爆炸">增量爆炸</h3><p>模型越深，对输出的扰动就越大。</p><h3 id="transformer-的pre-norm-和post-norm">transformer 的Pre Norm 和Post Norm</h3><p>一般来说，Pre Norm会比Post Norm更容易训练一些，但是Post Norm的最终效果往往更好。清华的一篇text-2-img文章CogView中，提出了一个三明治型的Norm方式，即同时使用Pre Norm 和 Post Norm。</p>]]></content>
    
    
    
    <tags>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>fast-slow-pointer</title>
    <link href="/2020/11/08/fast-slow-pointer/"/>
    <url>/2020/11/08/fast-slow-pointer/</url>
    
    <content type="html"><![CDATA[<p>???????????????</p><p>(image/)</p>]]></content>
    
    
    
    <tags>
      
      <tag>??</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用github搭建自己的博客</title>
    <link href="/2020/11/08/hello-world/"/>
    <url>/2020/11/08/hello-world/</url>
    
    <content type="html"><![CDATA[<p>下面记录一下我是如何利用GitHub来搭建自己的博客。</p><h2 id="准备工作">1. 准备工作</h2><ul><li>安装Node.js</li></ul><p>官网为：https://nodejs.org/zh-cn/ 建议下载安装长期支持版。安装后win+r打开终端，测试npm命令行工具能不能�? <pre><code class="hljs coffeescript"><span class="hljs-built_in">npm</span> -v</code></pre> 注：若已安装过Node.js，可以查看一下版本号，确保和下面要安装的Hexo兼容 <pre><code class="hljs crmsh"><span class="hljs-comment"># 查看Node.js版本</span><span class="hljs-keyword">node</span> <span class="hljs-title">-v</span></code></pre> <img src="https://cdn.jsdelivr.net/gh/zqp111/pic_bed/image/20210630162300.png" alt="20210630162300" /></p><ul><li>安装git</li></ul><p>这个相信大家应该都有，没有的话去 https://git-scm.com/download/win 下载安装</p><ul><li>安装Hexo</li></ul><p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p><pre><code class="hljs avrasm"><span class="hljs-meta"># 安装hexo</span>npm install -g hexo-<span class="hljs-keyword">cli</span></code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
